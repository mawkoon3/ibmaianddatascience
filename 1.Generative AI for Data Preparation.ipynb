{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "prev_pub_hash": "def6c2b16be03b8590d636aa576bdaff4206d1ae9e8a5ace4be932c0f896e5bb"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "<p style=\"text-align:center\">\n    <a href=\"https://skills.network\" target=\"_blank\">\n    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"Skills Network Logo\">\n    </a>\n</p>\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Test Environment for Generative AI classroom labs\n\nThis lab provides a test environment for the codes generated using the Generative AI classroom.\n\nFollow the instructions below to set up this environment for further use.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Setup\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Install required libraries\n\nIn case of a requirement of installing certain python libraries for use in your task, you may do so as shown below.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%pip install seaborn\nimport piplite\n\nawait piplite.install(['nbformat', 'plotly'])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "### Dataset URL from the GenAI lab\nUse the URL provided in the GenAI lab in the cell below. \n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "URL = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-Coursera/laptop_pricing_dataset_mod1.csv\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "### Downloading the dataset\n\nExecute the following code to download the dataset in to the interface.\n\n> Please note that this step is essential in JupyterLite. If you are using a downloaded version of this notebook and running it on JupyterLabs, then you can skip this step and directly use the URL in pandas.read_csv() function to read the dataset as a dataframe\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from pyodide.http import pyfetch\n\nasync def download(url, filename):\n    response = await pyfetch(url)\n    if response.status == 200:\n        with open(filename, \"wb\") as f:\n            f.write(await response.bytes())\n\npath = URL\n\nawait download(path, \"dataset.csv\")\nfile_name  = \"dataset.csv\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Test Environment\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Keep appending the code generated to this cell, or add more cells below this to execute in parts\n#This script reads a CSV file from the given path into a Pandas DataFrame. The first row is treated as headers by default (header=0).\n\nimport pandas as pd\n\n# Updated path to the CSV file (assuming it's in the current directory as \"dataset.csv\")\nfile_path = 'dataset.csv'  # Change this if your file is in a different location\n\ntry:\n    # Read the CSV into a DataFrame; the first row is used as column headers by default\n    df = pd.read_csv(file_path, header=0)\n    print(\"DataFrame loaded successfully!\")  # Optional confirmation\nexcept FileNotFoundError:\n    print(f\"Error: The file at '{file_path}' was not found. Please ensure the file exists in the current directory.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-4-648ef060c0f3>:4: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "DataFrame loaded successfully!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Assumes df is a pre-loaded DataFrame\n# Compute missing value counts per column\nmissing_counts = df.isnull().sum()\n\n# Identify columns that contain at least one missing value\ncolumns_with_missing = missing_counts[missing_counts > 0].index.tolist()\n\n# Prepare a compact result structure\nresult = {\n    'missing_counts_per_column': missing_counts.to_dict(),\n    'columns_with_missing': columns_with_missing\n}\n\n# Display results\nprint(\"Missing value counts per column:\")\nprint(missing_counts)\nprint(\"\\nColumns with missing values:\")\nprint(columns_with_missing)\n\nprint(\"\\nSummary:\")\nprint(result)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Missing value counts per column:\nUnnamed: 0        0\nManufacturer      0\nCategory          0\nScreen            0\nGPU               0\nOS                0\nCPU_core          0\nScreen_Size_cm    4\nCPU_frequency     0\nRAM_GB            0\nStorage_GB_SSD    0\nWeight_kg         5\nPrice             0\ndtype: int64\n\nColumns with missing values:\n['Screen_Size_cm', 'Weight_kg']\n\nSummary:\n{'missing_counts_per_column': {'Unnamed: 0': 0, 'Manufacturer': 0, 'Category': 0, 'Screen': 0, 'GPU': 0, 'OS': 0, 'CPU_core': 0, 'Screen_Size_cm': 4, 'CPU_frequency': 0, 'RAM_GB': 0, 'Storage_GB_SSD': 0, 'Weight_kg': 5, 'Price': 0}, 'columns_with_missing': ['Screen_Size_cm', 'Weight_kg']}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Assumes df is a pre-loaded DataFrame\n# First, check the columns in df to verify\nprint(\"Available columns in df:\", df.columns.tolist())  # This helps debug\n\n# Replace missing Screen_Size_cm with the most frequent value, if the column exists\nif 'Screen_Size_cm' in df.columns:\n    df['Screen_Size_cm'] = df['Screen_Size_cm'].fillna(df['Screen_Size_cm'].mode().iloc[0])\nelse:\n    print(\"Warning: Column 'Screen_Size_cm' does not exist in the DataFrame.\")\n\n# Replace missing Weight_kg with the mean value, if the column exists\nif 'Weight_kg' in df.columns:\n    df['Weight_kg'] = df['Weight_kg'].fillna(df['Weight_kg'].mean())\nelse:\n    print(\"Warning: Column 'Weight_kg' does not exist in the DataFrame.\")\n\nprint(\"DataFrame processing complete.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Available columns in df: ['Unnamed: 0', 'Manufacturer', 'Category', 'Screen', 'GPU', 'OS', 'CPU_core', 'Screen_Size_cm', 'CPU_frequency', 'RAM_GB', 'Storage_GB_SSD', 'Weight_kg', 'Price']\nDataFrame processing complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "# Assuming df is your processed DataFrame\ndf.to_csv(\"updated_data.csv\", index=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "print(\"Available columns in df:\", df.columns.tolist())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Available columns in df: ['Unnamed: 0', 'Manufacturer', 'Category', 'Screen', 'GPU', 'OS', 'CPU_core', 'Screen_Size_cm', 'CPU_frequency', 'RAM_GB', 'Storage_GB_SSD', 'Weight_kg', 'Price']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": "# Assuming df is already loaded and processed from previous steps\n\n# Check and convert Screen_Size_cm to float if the column exists\nif 'Screen_Size_cm' in df.columns:\n    df['Screen_Size_cm'] = df['Screen_Size_cm'].astype(float)\nelse:\n    print(\"Warning: Column 'Screen_Size_cm' does not exist in the DataFrame.\")\n\n# Check and convert Weight_kg to float if the column exists\nif 'Weight_kg' in df.columns:\n    df['Weight_kg'] = df['Weight_kg'].astype(float)\nelse:\n    print(\"Warning: Column 'Weight_kg' does not exist in the DataFrame.\")\n\nprint(\"Column conversions complete.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Column conversions complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": "# Assumes df is a pre-loaded DataFrame\n# Convert centimeters to inches and create a new column if 'Screen_Size_cm' exists\nif 'Screen_Size_cm' in df.columns:\n    df['Screen_Size_inch'] = df['Screen_Size_cm'] / 2.54\nelse:\n    print(\"Warning: Column 'Screen_Size_cm' does not exist in the DataFrame. Skipping conversion.\")\n\n# Convert kilograms to pounds and create a new column if 'Weight_kg' exists\nif 'Weight_kg' in df.columns:\n    df['Weight_pounds'] = df['Weight_kg'] * 2.2046226218\nelse:\n    print(\"Warning: Column 'Weight_kg' does not exist in the DataFrame. Skipping conversion.\")\n\n# Remove the original columns if they exist\ncolumns_to_drop = ['Screen_Size_cm', 'Weight_kg']\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns]  # Filter for existing columns\nif columns_to_drop:\n    df = df.drop(columns=columns_to_drop)\n    print(f\"Removed columns: {columns_to_drop}\")\nelse:\n    print(\"No columns to remove.\")\n\nprint(\"DataFrame processing complete.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Removed columns: ['Screen_Size_cm', 'Weight_kg']\nDataFrame processing complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "# Normalize CPU_frequency in place by scaling with the column max\n# This updates the existing column without creating a new attribute\ndf['CPU_frequency'] = df['CPU_frequency'] / df['CPU_frequency'].max()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\n\n# Assuming df is a pre-loaded DataFrame\n# Check if 'Screen' column exists before processing\nif 'Screen' in df.columns:\n    # Convert 'Screen' into one-hot encoded indicators with names 'Screen_<value>'\n    df1 = pd.get_dummies(df['Screen'], prefix='Screen')\n    \n    # Append the new indicator columns to the original DataFrame\n    df = pd.concat([df, df1], axis=1)\n    \n    # Drop the original 'Screen' column\n    df = df.drop(columns=['Screen'])\n    print(\"One-hot encoding and column drop completed successfully.\")\nelse:\n    print(\"Warning: Column 'Screen' does not exist in the DataFrame. Skipping processing.\")\n\nprint(\"DataFrame processing complete.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "One-hot encoding and column drop completed successfully.\nDataFrame processing complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "import os\nprint(\"Current directory:\", os.getcwd())",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Current directory: /drive/IBMSkillsNetwork-AI0271EN-SkillsNetwork/labs/test\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "# Assuming df is your processed DataFrame\ndf.to_csv(\"updated_data.csv\", index=False)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": "## Authors\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "[Abhishek Gagneja](https://www.linkedin.com/in/abhishek-gagneja-23051987/)\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Change Log\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n|-|-|-|-|\n|2023-12-10|0.1|Abhishek Gagneja|Initial Draft created|\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Copyright © 2023 IBM Corporation. All rights reserved.\n",
      "metadata": {}
    }
  ]
}